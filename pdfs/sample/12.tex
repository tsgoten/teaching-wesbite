\documentclass[10pt, twocolumn]{article}
\usepackage{../hw}
\title{Homework 12}
\author{Tarang Srivastava}

\begin{document}
\makechaptertitle

\section{Exercises 7.D}
\begin{q}[1]
    First we verify that the given operator
    $$ \sqrt{T^{*} T} v=\frac{\|x\|}{\|u\|}\langle v, u\rangle u $$
    is indeed positive. 
    Observe that when we calculate 
    $$ \ip{\sqrt{T^{*} T} v, v} = \frac{\|x\|}{\|u\|}|\langle v, u\rangle|^2 $$
    it is always a non-negative value, thus the operator satisfies the positivity condition.
    Now we wish to show that it is self adjoint, so observe thtat 
    $$  \ip{\sqrt{T^* T}v, u} = \frac{\|x\|}{\|u\|}\langle v, u\rangle \ip{u, u} $$
    $$  \ip{v, \sqrt{T^* T}u} = \frac{\|x\|}{\|u\|} \overline{\ip{u, u}} \ip{v, u} $$
    So the two inner products are the same, thus the given operator is self adjoint.
    Secondly, we can verify that the square is equal to the following value we will revisit
    \begin{align*}
        \sqrt{T^{*} T} \frac{\|x\|}{\|u\|}\langle v, u\rangle u &= \frac{\|x\|}{\|u\|}\langle v, u\rangle \sqrt{T^{*} T} u \\
        &= \frac{\|x\|}{\|u\|}\langle v, u\rangle \frac{\|x\|}{\|u\|}\langle u, u\rangle u \\
        &= \|x\|^2 \ip{v, u} u
    \end{align*}
    Third, we find the adjoint of $ T $. 
    Define $ T^* $ as follows
    $$ T^*v = \ip{v, x} u $$
    with the same fixed $ x $ and $ u $ as for $ T $.
    This is a valid adjoint since for $ v, w \in V $ we have
    \begin{align*}
        \ip{Tv, w} &= \ip{v, T^*w} \\
        \ip{v, u}\ip{x, w} &= \ip{v, u}\overline{\ip{w, x}} = \ip{v, u}\ip{x, w}
    \end{align*}
    For all $ v \in V $ we have that 
    \begin{align*}
        T^* T v &= T^*(\ip{v, u} x) \\
        &= \ip{v, u} \ip{x, x} u
    \end{align*}
    Then, we can verify that indeed $ \sqrt{T^* T}^2 = T^*T $ that we calculated earlier.
    Therefore, given that it is positive and self-adjiont it must be the unique positive square root of $ T^*T $.
\end{q}

\begin{q}[2]
    Let $ T $ be the operator represented by this $ 2 \times 2 $ matrix with the standard basis.
    $$
    T =
        \left(
        \begin{array}{cc}
            -5/2  & 5/2 \\
            -5/2 & 5/2
        \end{array}
        \right)
    $$
    The eigenvalues for $ T $ are exactly $ \lambda = 0 $.
    Then we find $ T^* T $ using the adjoint of the matrix provided. 
    $$ 
    T^* T = 
        \left(
        \begin{array}{cc}
            25/2  & -25/2 \\
            -25/2 & 25/2
        \end{array}
        \right)
    $$
    The eigenvalues for which are equal to $ \lambda = 25, 0 $. 
    So the singular values are equal to $ \sigma = 5, 0 $.
\end{q}

\begin{q}[4]
    Consider the polar decomposition for $ T $  
    $$ T = S \sqrt{T^* T}$$
    Then let $ v \in V $ be such that it is an eigenvector for $ \sqrt{T^* T} $ with the associated eigenvalue $ s $.
    So, 
    $$ Tv = S(sv) $$
    $$ \norm{Tv} = \norm{S(sv)} = \abs{s}\norm{Sv} $$
    Given that $ S $ is an isometry we then have
    $$ \norm{Tv} = s \norm{v} = s $$
    Since, $ s $ is nonnegative and the norm of $ v $ is 1. 
    By polar decomposition there always exists $ \sqrt{T^* T} $ thus an eigenvector associated with it.
\end{q}

\begin{q}[10]
    The singular values are the eigenvalues of $ \sqrt{\adjoint{T}T} $. 
    If $ T $ is self adjoint then $ \sqrt{\adjoint{T}T} = \sqrt{T^2} $. 
    From previous exercises we know that the eigenvalues for $ T^2 $ are just the eigenvalues $ \lambda $ for $ T $ squared, that is $ \lambda^2 $. 
    So we have that for all eigenvectors $ v $ of $ \sqrt{\adjoint{T}T} $
    $ \sqrt{T^2}v = sv $ such that $ T^2v = s^2v = \lambda^2 v $. 
    Taking the square root on both sides we get that $ s = \abs{s} = \abs{\lambda} $.
\end{q}

\begin{q}[11]
    First observe that the singular values for $ T^* $ are the eigenvalues of 
    $ \sqrt{TT^*} $.
    We know that $ T $ and $ \adjoint{T} $ have the same eigenvectors,
    and the associated eigenvalues are $ \lambda $ and $ \overline{\lambda} $ respectively. 
    Then for some eigenvector we have 
    $$ T^*Tv = T^*\lambda v = \abs{\lambda}^2 v $$
    also
    $$ TT^*v = T^*\overline{\lambda} v = \abs{\lambda}^2 v $$
    Since, $ TT^* $ and $ \adjoint{T}T $ have the same positive eigenvalues. 
    Their, singular values are the same, equal to the positive square root of those same eigenvalues.
\end{q}

\begin{q}[12]
    Consider the vector space $ V = \F^2 $ and the operator $ T $ represented by the following matrix in the standard basis.
    $$
    T = 
        \left(
            \begin{array}{cc}
                0 & 0 \\
                -1 & 0 \\ 
            \end{array}
        \right)
    $$
    Then this operator has the singular values $ \sigma = 1, 0 $.
    Observe that, $ T^2 = 0 $, so it has the singular values $ \sigma = 0 $ 
    So, the singular values aren't even equal. 
\end{q}

\begin{q}[13]
    $ \implies $
    First note that $ \sqrt{0} = 0 = 0^2 $.
    If $ T $ is invertible, then $$ \nul \adjoint{T} = (\range T)^\perp = V^\perp = \emptyspace $$
    So, $ T^* $ is invertible, and then $ T^*T $ is invertible.  
    Then from a pervious exercise, $ T^* T $ does not have zero as an eigenvalue.
    Since the singular values are the positive square root of the eigenvalues of $ \adjoint{T} T $ and it has non-zero eigenvalues, 
    then the singular values are all non-zero as well. \\
    $ \impliedby $ Consider the polar decomposition of $ T $. 
    If all the singular values are nonzero, then $ \sqrt{\adjoint{T} T} $ has all nonzero eigenvalues.
    Then, $ \nul \sqrt{\adjoint{T} T} = \emptyspace $ so it is inveritble. 
    Since, $ S $ is an isometry and we have a composition of invertible operators $ S $ and $ \sqrt{\adjoint{T} T} $ which is invertible, 
    Therefore, $ T $ is invertible.
\end{q}

\pagebreak
\section{8.A}

\begin{q}[3]
    Let $ n $ be the dimension of the vector space.
    Consider the subspace $ \nul (T - \lambda I)^n $, 
    and $ \nul (\inv{T} - \frac{1}{\lambda} I)^n $. 
    It follows that each is equal to 
    $ G(\lambda, T) $ and $ G(\inv{\lambda}, \inv{T}) $ respectively. \\
    Let $ v \in \nul (T - \lambda I)^n $, then
    \begin{align*}
        (T - \lambda I)^n v &= 0 \\
        \intertext{Since, $ \inv{T} T = I $ and the inverse and the identity operator commute.}
        \left(\inv{T} - \frac{1}{\lambda}I \right)(-\lambda)^nT^n v &= 0\\
        \intertext{Since the null space of $ T^n $ is the empty vector space, 
        it must be that $ v $ is in the null space of the other operators in the term. That is}
        \left(\inv{T} - \frac{1}{\lambda}I \right)v &= 0 
        \intertext{Thus,  $\nul (T - \lambda I)^n \subset \nul (\inv{T} - \frac{1}{\lambda} I)^n$. 
        Now suppose we take $ v \in \nul (\inv{T} - \inv{\lambda} I)^n $}
        \left(\inv{T} - \frac{1}{\lambda}I \right)v &= 0  \\
        (T - \lambda I)^n \left(- \frac{1}{\lambda}\right)^n (\inv{T})^n v &= 0 \\
        \intertext{Since the null space of $ (\inv{T})^n $ is the empty vector space, 
        it must be that $ v $ is in the null space of the other operators in the term. That is}
        (T - \lambda I)^n v &= 0
        \intertext{Thus,  $\nul (T - \lambda I)^n \supset \nul (\inv{T} - \frac{1}{\lambda} I)^n$.}
    \end{align*}
    Sof finally we have that the two general eigenspaces are equal.
\end{q}

\begin{q}[4]
    Assume for contradiction that the intersection is not empty, 
    $$ G(\alpha, T) \cap G(\beta, T) \neq \emptyspace $$
    Let $ v $ be the eigenvector in the intersection
    $$ v \in G(\alpha, T) \cap G(\beta, T) $$
    Then construct the linearly independent list of eigenvectors as described in 8.13, choosing $ v $ to represent both $ \alpha $ and $ \beta $. 
    It is trivial now that that the list is not linearly independent, thus a contradiction.
\end{q}

\begin{q}[6]
    Assume for contradiction that there exists a $ S \in \Operator(\C^3) $
    such that $ S^2 = T $. 
    We can see that $ T^3 = 0 $, so $ S^6 = 0 $. 
    But since the dimension of the vector space is 3 we have that
    $$ \nul S^3 = ... = \nul S^6 $$
    Specifically this means that $ \nul S^4 = \nul S^6 = \C^3 $. 
    But this is a contradiction since we know that 
    $$ T^2 \neq 0 $$
    So $ \nul S^4 \neq \nul T^2 $. 
\end{q}

\begin{q}[8]
    No! Consider the vector space $ \C^2 $ 
    and the operators $ S $ and $ T $ given by following matrices respectively
    $$ 
        \left(
            \begin{array}{cc}
                0 & 1 \\
                0 & 0 
            \end{array}
        \right)
        \text{ and }
        \left(
            \begin{array}{cc}
                0 & 0 \\
                1 & 0 
            \end{array}
        \right)
    $$
    in the standard basis for $ \C^2 $.
    It's easy to see that $ S^2 = 0 $ and $ T^2 = 0 $, so they both are nilpotent operators.
    Then it easily follows that the set of nilpotent operators is not closed under vector addition, 
    because for $ S + T $ given by the matrix
    $$
        \left(
            \begin{array}{cc}
                0 & 1 \\
                1 & 0 
            \end{array}
        \right)
    $$
    We have that $ (S + T)^2 = I $. 
    From there it holds that for all $ n \in \N $, 
    $$ (S + T)^n = I^{n - 1} = I $$
    so it is never equal to zero and thus not nilpotent.
\end{q}

\begin{q}[9]
    We know that $ ST $ and $ TS $ have the same eigenvalues. 
    Since, $ ST $ is nilpotent, 
    there is a diagaonl matrix of $ ST $ with all zeros on the diagaonl. 
    So the one and only eigenvalue of $ ST $ is $ \lambda = 0 $.     
\end{q}

\begin{q}[11]
    Let $ n = \dim V $, we know that $ (ST)^n = 0 $. 
    Then, 
    \begin{align*}
        S &= S  \\
        (ST)^n S &= 0 S = 0 \\
        T (ST)^n S &= T 0 = 0 \\
        T (ST)(ST)...(ST)(ST)S &= 0
        \intertext{then we regroup all the operators to get}
        (TS)(TS)...(TS) &= 0 
        (TS)^{n + 1} &= 0
    \end{align*}
    Then we know that for any operator raised to a power greater than the dimension it is unchanged, 
    so $ (TS)^n = (TS)^{n + 1} = 0 $. 
    Therefore, $ TS $ is nilpotent.
\end{q}

\begin{q}[14]
    TL;DR: $ N \implies 8.19 \implies 6.37 $. \\
    By 8.19 $ N $ has an upper triangular matrix with all zeros on the diagonal. 
    Then by 6.37, given $ N $ has an upper triangular matrix in some basis, 
    $ N $ has an upper triangular matrix in an orthonormal basis. 
\end{q}

\end{document}